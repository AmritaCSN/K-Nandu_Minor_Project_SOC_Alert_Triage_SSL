{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid SSL + Drift Adaptation**\n",
    "## **Final Model Training Pipeline**\n",
    "\n",
    "This notebook implements the final solution for the SOC Alert Triage project. It addresses the **concept drift** issue (where Friday traffic differs from Mon-Thu) by creating a \"Hybrid Seed.\"\n",
    "\n",
    "### **The Hybrid Strategy:**\n",
    "1.  **Labeled Seed (The Teacher):** Composed of **30% Mon-Thu data** AND **15% Friday data**. This \"Drift Bridge\" teaches the model the new attack patterns immediately.\n",
    "2.  **Unlabeled Pool (The Amplifier):** **70% of Mon-Thu data**. We use Semi-Supervised Learning (Tri-Training) to leverage this massive volume of data to stabilize the decision boundaries.\n",
    "3.  **Test Set (The Exam):** **85% of Friday data**. This data is completely hidden during training and is used solely to evaluate final performance.\n",
    "\n",
    "### **Goal:**\n",
    "Achieve **>99% Recall** on the Friday test set while maintaining a high Auto-Close rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM detected. Using Gradient Boosting.\n",
      "Configuration Loaded. Reading from: C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, recall_score, precision_score, \n",
    "    confusion_matrix, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing LightGBM (Faster/Better), fall back to RandomForest\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "    print(\"✅ LightGBM detected. Using Gradient Boosting.\")\n",
    "except ImportError:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    HAS_LGBM = False\n",
    "    print(\"⚠️ LightGBM not found. Falling back to Random Forest.\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "ROOT_DIR = r\"C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\"\n",
    "PROCESSED_DIR = os.path.join(ROOT_DIR, \"processed\")\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MIN_CONF_FLOOR = 0.05   # Minimum confidence to consider a pseudo-label\n",
    "MIN_TOP_K = 1000        # Number of pseudo-labels to add per round\n",
    "\n",
    "print(f\"Configuration Loaded. Reading from: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Class Definitions**\n",
    "Here we define the **Rank-Based Tri-Training** class. This custom SSL algorithm uses three classifiers that \"teach\" each other. If two models agree on a label with high confidence, they teach the third model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_builder(seed=42):\n",
    "    \"\"\"Factory function to build base classifiers.\"\"\"\n",
    "    if HAS_LGBM:\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=200, \n",
    "            learning_rate=0.05, \n",
    "            num_leaves=31,\n",
    "            class_weight='balanced', \n",
    "            random_state=seed, \n",
    "            n_jobs=-1, \n",
    "            verbose=-1\n",
    "        )\n",
    "    else:\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            class_weight='balanced', \n",
    "            random_state=seed, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "class RankTriTraining:\n",
    "    \"\"\"Rank-Based Tri-Training with Probability Calibration\"\"\"\n",
    "    def __init__(self, base_builder, n_models=3, max_iter=5, top_k=1000, \n",
    "                 min_conf=0.05, random_state=42):\n",
    "        self.base_builder = base_builder\n",
    "        self.n_models = n_models\n",
    "        self.max_iter = max_iter\n",
    "        self.top_k = top_k\n",
    "        self.min_conf_floor = min_conf\n",
    "        self.random_state = random_state\n",
    "        self.models = [] \n",
    "        self.train_data = [] \n",
    "\n",
    "    def fit(self, X_labeled, y_labeled, X_unlabeled, X_cal, y_cal):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.models = []\n",
    "        self.train_data = []\n",
    "\n",
    "        # --- Initialization Phase ---\n",
    "        print(f\"[Init] Training {self.n_models} base models on Labeled Seed...\")\n",
    "        for i in range(self.n_models):\n",
    "            # Bootstrap sample for diversity\n",
    "            idx = rng.choice(len(X_labeled), size=int(0.8 * len(X_labeled)), replace=True)\n",
    "            X_boot = X_labeled[idx]\n",
    "            y_boot = y_labeled[idx]\n",
    "            \n",
    "            base = self.base_builder(seed=self.random_state + i)\n",
    "            base.fit(X_boot, y_boot)\n",
    "            \n",
    "            # Calibrate probabilities using Isotonic Regression\n",
    "            cal = CalibratedClassifierCV(base, cv='prefit', method='isotonic')\n",
    "            cal.fit(X_cal, y_cal)\n",
    "            self.models.append(cal)\n",
    "            self.train_data.append([X_boot, y_boot])\n",
    "\n",
    "        # --- Tri-Training Loop ---\n",
    "        X_u = np.array(X_unlabeled)\n",
    "        mask = np.ones(len(X_u), dtype=bool) # Track available unlabeled data\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            print(f\"--- Iteration {iteration + 1}/{self.max_iter} ---\")\n",
    "            pool_idx = np.where(mask)[0]\n",
    "            if len(pool_idx) == 0: break\n",
    "            \n",
    "            X_pool = X_u[pool_idx]\n",
    "            # Get predictions from all models\n",
    "            probs = [m.predict_proba(X_pool)[:, 1] for m in self.models]\n",
    "            preds = [(p >= 0.5).astype(int) for p in probs]\n",
    "\n",
    "            candidates = {0: [], 1: [], 2: []}\n",
    "            pairs = [(0, 1, 2), (0, 2, 1), (1, 2, 0)] # (Teacher1, Teacher2, Student)\n",
    "            \n",
    "            for i, j, k in pairs:\n",
    "                # Where do Teachers (i, j) agree?\n",
    "                agree_idx = np.where(preds[i] == preds[j])[0]\n",
    "                if len(agree_idx) == 0: continue\n",
    "                \n",
    "                # Calculate Confidence (Distance from 0.5)\n",
    "                conf_i = np.abs(probs[i][agree_idx] - 0.5)\n",
    "                conf_j = np.abs(probs[j][agree_idx] - 0.5)\n",
    "                min_conf = np.minimum(conf_i, conf_j)\n",
    "                \n",
    "                # Filter by Floor & Rank by Confidence\n",
    "                valid_mask = min_conf >= self.min_conf_floor\n",
    "                valid_idx = agree_idx[valid_mask]\n",
    "                valid_conf = min_conf[valid_mask]\n",
    "                \n",
    "                sorted_idx = np.argsort(-valid_conf)[:self.top_k]\n",
    "                top_indices = valid_idx[sorted_idx]\n",
    "                \n",
    "                # Store Pseudo-Labels for Student k\n",
    "                for loc_idx in top_indices:\n",
    "                    g_idx = pool_idx[loc_idx]\n",
    "                    lbl = preds[i][loc_idx]\n",
    "                    candidates[k].append((g_idx, lbl))\n",
    "\n",
    "            # Update Models with New Pseudo-Labels\n",
    "            added = 0\n",
    "            to_remove = set()\n",
    "            for k in range(self.n_models):\n",
    "                if not candidates[k]: continue\n",
    "                unique = {x[0]: x[1] for x in candidates[k]}\n",
    "                if not unique: continue\n",
    "                \n",
    "                new_X = X_u[list(unique.keys())]\n",
    "                new_y = np.array(list(unique.values()))\n",
    "                \n",
    "                self.train_data[k][0] = np.vstack([self.train_data[k][0], new_X])\n",
    "                self.train_data[k][1] = np.concatenate([self.train_data[k][1], new_y])\n",
    "                \n",
    "                # Retrain & Recalibrate\n",
    "                base = self.base_builder(seed=self.random_state + iteration)\n",
    "                base.fit(self.train_data[k][0], self.train_data[k][1])\n",
    "                cal = CalibratedClassifierCV(base, cv='prefit', method='isotonic')\n",
    "                cal.fit(X_cal, y_cal)\n",
    "                self.models[k] = cal\n",
    "                \n",
    "                added += len(new_X)\n",
    "                to_remove.update(unique.keys())\n",
    "            \n",
    "            mask[list(to_remove)] = False\n",
    "            print(f\"  Added {added} pseudo-labels. Remaining Pool: {mask.sum()}\")\n",
    "            if added == 0: break\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Ensemble Average Prediction\n",
    "        probs = np.array([m.predict_proba(X)[:, 1] for m in self.models])\n",
    "        return np.mean(probs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data Loading & Hybrid Splitting**\n",
    "This is the most critical step. We construct the training sets to include the \"Drift Bridge.\"\n",
    "\n",
    "**The Splits:**\n",
    "* **Mon-Thu Data:** 30% goes to Labeled Seed, 70% goes to Unlabeled Pool.\n",
    "* **Friday Data:** 15% goes to Labeled Seed (The Bridge), 85% goes to Hidden Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " FINAL: HYBRID SSL + DRIFT ADAPTATION\n",
      "================================================================================\n",
      "\n",
      "[2] Constructing Hybrid Datasets...\n",
      "  Labeled Seed (Total):   743,735\n",
      "     ├─ From Mon-Thu:     638,249\n",
      "     └─ From Friday:      105,486 (The Drift Bridge)\n",
      "  Unlabeled Pool:         1,489,249 (Mon-Thu Only)\n",
      "  Test Set (Friday):      597,759\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" FINAL: HYBRID SSL + DRIFT ADAPTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Load Data\n",
    "train_df = pd.read_parquet(os.path.join(PROCESSED_DIR, \"train_mon_thu.parquet\"))\n",
    "test_df = pd.read_parquet(os.path.join(PROCESSED_DIR, \"test_friday.parquet\"))\n",
    "\n",
    "drop_cols = {'label', 'label_str', 'timestamp', 'source_file', 'src_ip', 'dst_ip'}\n",
    "feature_cols = [c for c in train_df.columns if c not in drop_cols]\n",
    "\n",
    "X_mon_thu = train_df[feature_cols].values\n",
    "y_mon_thu = train_df['label'].values\n",
    "X_fri = test_df[feature_cols].values\n",
    "y_fri = test_df['label'].values\n",
    "\n",
    "# 2. CREATE THE HYBRID SPLITS (The Magic Step)\n",
    "print(\"\\n[2] Constructing Hybrid Datasets...\")\n",
    "\n",
    "# A. Split Mon-Thu: 30% Seed, 70% Unlabeled Pool\n",
    "X_mt_lbl, X_mt_unlab, y_mt_lbl, y_mt_unlab = train_test_split(\n",
    "    X_mon_thu, y_mon_thu, test_size=0.70, stratify=y_mon_thu, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# B. Split Friday: 15% Seed (Drift Teach), 85% Test (Hidden)\n",
    "X_fri_lbl, X_fri_test, y_fri_lbl, y_fri_test = train_test_split(\n",
    "    X_fri, y_fri, test_size=0.85, stratify=y_fri, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# C. Combine Seeds (Mon-Thu + Fri Slice)\n",
    "X_seed = np.vstack([X_mt_lbl, X_fri_lbl])\n",
    "y_seed = np.concatenate([y_mt_lbl, y_fri_lbl])\n",
    "\n",
    "# D. Create Calibration Set (10% of the Seed)\n",
    "# We hold out a small chunk of the seed to calibrate probabilities (important for Tri-Training)\n",
    "X_train_seed, X_cal, y_train_seed, y_cal = train_test_split(\n",
    "    X_seed, y_seed, test_size=0.10, stratify=y_seed, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"  Labeled Seed (Total):   {len(X_seed):,}\")\n",
    "print(f\"     ├─ From Mon-Thu:     {len(X_mt_lbl):,}\")\n",
    "print(f\"     └─ From Friday:      {len(X_fri_lbl):,} (The Drift Bridge)\")\n",
    "print(f\"  Unlabeled Pool:         {len(X_mt_unlab):,} (Mon-Thu Only)\")\n",
    "print(f\"  Test Set (Friday):      {len(X_fri_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Training & Evaluation**\n",
    "We now run the Tri-Training loop. After training, we dynamically calculate a **Safety Threshold** on the training data (aiming for 99.9% Recall) and apply it to the unseen Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] Running Tri-Training on Hybrid Data...\n",
      "[Init] Training 3 base models on Labeled Seed...\n",
      "--- Iteration 1/5 ---\n",
      "  Added 3000 pseudo-labels. Remaining Pool: 1488193\n",
      "--- Iteration 2/5 ---\n",
      "  Added 3000 pseudo-labels. Remaining Pool: 1487147\n",
      "--- Iteration 3/5 ---\n",
      "  Added 3000 pseudo-labels. Remaining Pool: 1486071\n",
      "--- Iteration 4/5 ---\n",
      "  Added 3000 pseudo-labels. Remaining Pool: 1485005\n",
      "--- Iteration 5/5 ---\n",
      "  Added 3000 pseudo-labels. Remaining Pool: 1483879\n",
      "\n",
      "[4] Evaluating on Hidden Friday Test Set...\n",
      "  Safety Threshold: 0.384259\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS (HYBRID APPROACH)\n",
      "================================================================================\n",
      "Safety Threshold Used: 0.384259\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                    | Score      | Impact on SOC                 \n",
      "--------------------------------------------------------------------------------\n",
      "Recall (Attack)           | 99.81%     | Catching Attacks (Target >99%)\n",
      "Precision                 | 99.83%     | Reducing False Alarms\n",
      "F1-Score                  | 0.9982     | Overall Model Quality\n",
      "ROC-AUC                   | 1.0000     | Threshold Independence\n",
      "--------------------------------------------------------------------------------\n",
      "False Negative Rate       | 0.19%     | Missed Attacks (Critical Risk)\n",
      "False Positive Rate       | 0.12%     | Alert Credibility\n",
      "--------------------------------------------------------------------------------\n",
      "Auto-Close Rate           | 58.92%     | Automation Benefit\n",
      "Analyst Workload          | 41.08%     | Alert Fatigue Ratio (Remaining)\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix:   TN=351763 | FP=411 | FN=460 | TP=245125\n",
      "\n",
      "Model saved to: C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\models\\final_hybrid_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# 3. Train Hybrid Model\n",
    "print(\"\\n[3] Running Tri-Training on Hybrid Data...\")\n",
    "tri_model = RankTriTraining(\n",
    "    base_builder, n_models=3, max_iter=5, top_k=MIN_TOP_K, min_conf=MIN_CONF_FLOOR\n",
    ")\n",
    "tri_model.fit(X_train_seed, y_train_seed, X_mt_unlab, X_cal, y_cal)\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"\\n[4] Evaluating on Hidden Friday Test Set...\")\n",
    "probs = tri_model.predict_proba(X_fri_test)\n",
    "\n",
    "# 5. Drift-Adaptive Threshold\n",
    "# We compute threshold on the Training Seed (which now includes Friday examples)\n",
    "probs_seed = tri_model.predict_proba(X_train_seed)\n",
    "prec, rec, thresh = precision_recall_curve(y_train_seed, probs_seed)\n",
    "\n",
    "# Find threshold where Recall >= 99.9% on Training Data\n",
    "target_idx = np.where(rec >= 0.999)[0]\n",
    "safety_thresh = thresh[target_idx[-1]] if len(target_idx) > 0 else 0.001\n",
    "\n",
    "print(f\"  Safety Threshold: {safety_thresh:.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. COMPREHENSIVE METRICS & REPORTING\n",
    "# =============================================================================\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "# 1. Apply Safety Threshold\n",
    "preds = (probs >= safety_thresh).astype(int)\n",
    "\n",
    "# 2. Confusion Matrix Elements\n",
    "tn, fp, fn, tp = confusion_matrix(y_fri_test, preds).ravel()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "final_rec = recall_score(y_fri_test, preds)      # Ability to catch attacks\n",
    "final_prec = precision_score(y_fri_test, preds)  # Trustworthiness of alerts\n",
    "final_f1 = f1_score(y_fri_test, preds)           # Harmonic mean (Balance)\n",
    "final_roc = roc_auc_score(y_fri_test, probs)     # Model discrimination power\n",
    "\n",
    "# 4. Error Rates\n",
    "final_fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0  # Miss rate (Critical Risk)\n",
    "final_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0  # False Alarm Rate (Noise)\n",
    "\n",
    "# 5. Operational Metrics (SOC ROI)\n",
    "# Auto-Close: Alerts below threshold are considered \"Benign\" and auto-closed\n",
    "auto_close_mask = probs < safety_thresh\n",
    "auto_close_rate = np.mean(auto_close_mask)\n",
    "\n",
    "# Analyst Workload: Alerts above threshold sent for human review\n",
    "analyst_workload_ratio = 1.0 - auto_close_rate\n",
    "\n",
    "# --- PRINT PROFESSIONAL REPORT ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS (HYBRID APPROACH)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Safety Threshold Used: {safety_thresh:.6f}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<25} | {'Score':<10} | {'Impact on SOC':<30}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Recall (Attack)':<25} | {final_rec:.2%}     | Catching Attacks (Target >99%)\")\n",
    "print(f\"{'Precision':<25} | {final_prec:.2%}     | Reducing False Alarms\")\n",
    "print(f\"{'F1-Score':<25} | {final_f1:.4f}     | Overall Model Quality\")\n",
    "print(f\"{'ROC-AUC':<25} | {final_roc:.4f}     | Threshold Independence\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'False Negative Rate':<25} | {final_fnr:.2%}     | Missed Attacks (Critical Risk)\")\n",
    "print(f\"{'False Positive Rate':<25} | {final_fpr:.2%}     | Alert Credibility\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Auto-Close Rate':<25} | {auto_close_rate:.2%}     | Automation Benefit\")\n",
    "print(f\"{'Analyst Workload':<25} | {analyst_workload_ratio:.2%}     | Alert Fatigue Ratio (Remaining)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Confusion Matrix:   TN={tn} | FP={fp} | FN={fn} | TP={tp}\")\n",
    "\n",
    "# Save Model\n",
    "joblib.dump(tri_model, os.path.join(MODEL_DIR, \"final_hybrid_model.joblib\"))\n",
    "print(f\"\\nModel saved to: {os.path.join(MODEL_DIR, 'final_hybrid_model.joblib')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
