{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project Preprocessing: CICIDS2017 Data Preparation**\n",
    "## **Zero Data Leakage Pipeline**\n",
    "\n",
    "This notebook implements the data preprocessing pipeline for the SOC Alert Triage optimization project. It transforms raw network traffic logs (CICIDS2017) into a clean, feature-rich dataset ready for machine learning.\n",
    "\n",
    "### **Pipeline Stages:**\n",
    "1.  **Data Loading & Temporal Splitting:**\n",
    "    * **Training Set:** Monday to Thursday traffic (Baseline behavior + initial attacks).\n",
    "    * **Test Set:** Friday traffic (Distinctly different attack patterns to test generalization).\n",
    "2.  **Data Cleaning:** Standardizing column names, fixing labels, and handling infinite/NaN values.\n",
    "3.  **Stateless Feature Engineering:** creating ratios and flag aggregates derived from single-row attributes.\n",
    "4.  **Stateful Feature Engineering (Zero Leakage):** Learning distribution stats (like Port Rarity) *only* from the training set and applying them to the test set.\n",
    "5.  **Anomaly Detection Feature:** Training an Isolation Forest on benign training data to generate an `anomaly_score` feature for downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded.\n",
      "Root: C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\n",
      "Data Source: C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# TODO: Update ROOT_DIR to your specific project path if running locally\n",
    "ROOT_DIR = r\"C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\" \n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")      # Folder containing raw CSV files\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"processed\")\n",
    "ARTIFACTS_DIR = os.path.join(ROOT_DIR, \"models\", \"feature_artifacts\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration Loaded.\")\n",
    "print(f\"Root: {ROOT_DIR}\")\n",
    "print(f\"Data Source: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# UTILITY FUNCTIONS\n",
    "# -----------------------\n",
    "def clean_col_name(name):\n",
    "    \"\"\"\n",
    "    Standardize column names: lowercase, remove spaces and special characters.\n",
    "    Example: \" Flow Duration \" -> \"flow_duration\"\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^0-9a-zA-Z_]+\", \"\", name.strip().lower().replace(\" \", \"_\"))\n",
    "\n",
    "def get_day_from_filename(filepath):\n",
    "    \"\"\"\n",
    "    Extract day index from filename (Case Insensitive).\n",
    "    Returns: 0=Mon, 1=Tue, 2=Wed, 3=Thu, 4=Fri, -1=Unknown\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(filepath).lower()\n",
    "    if \"monday\" in fname: return 0\n",
    "    if \"tuesday\" in fname: return 1\n",
    "    if \"wednesday\" in fname: return 2\n",
    "    if \"thursday\" in fname: return 3\n",
    "    if \"friday\" in fname: return 4\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 1: Load CSVs and Temporal Split**\n",
    "To simulate a real-world SOC environment, we split data by **time** rather than randomly shuffling it. \n",
    "* **Training:** Monday - Thursday\n",
    "* **Testing:** Friday\n",
    "\n",
    "This ensures the model is tested on \"future\" data it has never seen, preventing look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[STAGE 1] Loading CSVs and Splitting by Day (Mon-Thu vs Fri)...\n",
      "================================================================================\n",
      "Found 8 CSV files in C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\data\n",
      "\n",
      "   Processing: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "    -> Detected: Friday\n",
      "    ✓ Added to TEST set (Friday)\n",
      "\n",
      "   Processing: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "    -> Detected: Friday\n",
      "    ✓ Added to TEST set (Friday)\n",
      "\n",
      "   Processing: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "    -> Detected: Friday\n",
      "    ✓ Added to TEST set (Friday)\n",
      "\n",
      "   Processing: Monday-WorkingHours.pcap_ISCX.csv\n",
      "    -> Detected: Monday\n",
      "    ✓ Added to TRAIN set (Mon-Thu)\n",
      "\n",
      "   Processing: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "    -> Detected: Thursday\n",
      "    ✓ Added to TRAIN set (Mon-Thu)\n",
      "\n",
      "   Processing: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "    -> Detected: Thursday\n",
      "    ✓ Added to TRAIN set (Mon-Thu)\n",
      "\n",
      "   Processing: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "    -> Detected: Tuesday\n",
      "    ✓ Added to TRAIN set (Mon-Thu)\n",
      "\n",
      "   Processing: Wednesday-workingHours.pcap_ISCX.csv\n",
      "    -> Detected: Wednesday\n",
      "    ✓ Added to TRAIN set (Mon-Thu)\n",
      "\n",
      "Concatenating DataFrames...\n",
      "\n",
      "✓ Train (Mon-Thu): 2,127,498 rows | Attack Rate: 12.63%\n",
      "✓ Test (Friday):   703,245 rows | Attack Rate: 41.08%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"[STAGE 1] Loading CSVs and Splitting by Day (Mon-Thu vs Fri)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
    "print(f\"Found {len(all_files)} CSV files in {DATA_DIR}\")\n",
    "\n",
    "if not all_files:\n",
    "    # If running in Colab without mounting drive, this might fail\n",
    "    print(\"WARNING: No files found. Please check your DATA_DIR path.\")\n",
    "else:\n",
    "    train_list = [] # Mon-Thu\n",
    "    test_list = []  # Fri\n",
    "\n",
    "    for fpath in all_files:\n",
    "        fname = os.path.basename(fpath)\n",
    "        print(f\"\\n   Processing: {fname}\")\n",
    "        \n",
    "        # 1. Determine Day First\n",
    "        day_idx = get_day_from_filename(fpath)\n",
    "        day_names = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", -1: \"Unknown\"}\n",
    "        print(f\"    -> Detected: {day_names.get(day_idx, 'Unknown')}\")\n",
    "\n",
    "        # 2. Load with encoding fallback\n",
    "        try:\n",
    "            df = pd.read_csv(fpath, encoding='cp1252', low_memory=False)\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(fpath, encoding='utf-8', low_memory=False)\n",
    "            except:\n",
    "                print(f\"    WARNING: Failed to load {fname}. Skipping.\")\n",
    "                continue\n",
    "        \n",
    "        # 3. Clean Columns\n",
    "        df.columns = [clean_col_name(c) for c in df.columns]\n",
    "\n",
    "        # 4. Fix Label (0=Benign, 1=Attack)\n",
    "        # Checks for 'label' or 'label_str' columns\n",
    "        label_col = next((c for c in df.columns if \"label\" in c), None)\n",
    "        if label_col:\n",
    "            # Standardize 'benign' to 0, everything else (attacks) to 1\n",
    "            df['label'] = df[label_col].apply(lambda x: 0 if str(x).lower() == 'benign' else 1)\n",
    "        else:\n",
    "            print(f\"    WARNING: No label column found!\")\n",
    "            continue\n",
    "\n",
    "        # 5. Fix Ports (Critical for Rarity Feature)\n",
    "        for p in ['dst_port', 'destination_port']:\n",
    "            if p in df.columns:\n",
    "                df['dst_port'] = pd.to_numeric(df[p], errors='coerce').fillna(-1).astype(int)\n",
    "                break\n",
    "\n",
    "        # 6. Clean Numerics (Inf -> NaN -> 0)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # 7. Append to appropriate list\n",
    "        if day_idx == 4: # Friday\n",
    "            test_list.append(df)\n",
    "            print(f\"    ✓ Added to TEST set (Friday)\")\n",
    "        elif day_idx >= 0: # Mon-Thu\n",
    "            train_list.append(df)\n",
    "            print(f\"    ✓ Added to TRAIN set (Mon-Thu)\")\n",
    "        else:\n",
    "            print(f\"    ✗ Skipped (Unknown day)\")\n",
    "        \n",
    "        # Memory management\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    # Validation & Concatenation\n",
    "    if not train_list:\n",
    "        print(\"ERROR: TRAIN LIST IS EMPTY! No Mon-Thu files were detected.\")\n",
    "    elif not test_list:\n",
    "        print(\"ERROR: TEST LIST IS EMPTY! No Friday files were detected.\")\n",
    "    else:\n",
    "        print(\"\\nConcatenating DataFrames...\")\n",
    "        train_df = pd.concat(train_list, ignore_index=True)\n",
    "        test_df = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "        # Free up memory\n",
    "        del train_list, test_list\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"\\n✓ Train (Mon-Thu): {len(train_df):,} rows | Attack Rate: {train_df['label'].mean():.2%}\")\n",
    "        print(f\"✓ Test (Friday):   {len(test_df):,} rows | Attack Rate: {test_df['label'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 2: Stateless Feature Engineering**\n",
    "These features are calculated row-by-row without needing knowledge of the dataset distribution.\n",
    "\n",
    "**Key Features Added:**\n",
    "* **Payload Ratios:** Bytes per packet (Forward/Backward).\n",
    "* **Flag Ratios:** SYN, RST, and ACK ratios (useful for detecting scanning and DoS attacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STAGE 2] Stateless Feature Engineering...\n",
      "================================================================================\n",
      "Adding stateless features to TRAIN...\n",
      "Adding stateless features to TEST...\n",
      "✓ Stateless features added.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STAGE 2] Stateless Feature Engineering...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def add_stateless_features(df):\n",
    "    \"\"\"Create features that don't depend on global statistics.\"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "    \n",
    "    # 1. Payload Ratios\n",
    "    if 'subflow_fwd_bytes' in df.columns and 'total_fwd_packets' in df.columns:\n",
    "        df['bytes_per_fwd_packet'] = df['subflow_fwd_bytes'] / (df['total_fwd_packets'] + eps)\n",
    "    if 'subflow_bwd_bytes' in df.columns and 'total_backward_packets' in df.columns:\n",
    "        df['bytes_per_bwd_packet'] = df['subflow_bwd_bytes'] / (df['total_backward_packets'] + eps)\n",
    "    if 'avg_fwd_segment_size' in df.columns and 'avg_bwd_segment_size' in df.columns:\n",
    "        df['avg_seg_ratio'] = df['avg_fwd_segment_size'] / (df['avg_bwd_segment_size'] + eps)\n",
    "        \n",
    "    # 2. Flag Anomalies (Scanning/DoS Detection)\n",
    "    flag_cols = [\"fin_flag_count\", \"syn_flag_count\", \"rst_flag_count\", \"ack_flag_count\"]\n",
    "    for f in flag_cols:\n",
    "        if f not in df.columns: \n",
    "            df[f] = 0.0\n",
    "    \n",
    "    flags_sum = df[flag_cols].sum(axis=1) + eps\n",
    "    df[\"syn_ratio\"] = df[\"syn_flag_count\"] / flags_sum\n",
    "    df[\"rst_ratio\"] = df[\"rst_flag_count\"] / flags_sum\n",
    "    df[\"ack_ratio\"] = df[\"ack_flag_count\"] / flags_sum\n",
    "    \n",
    "    return df\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    print(\"Adding stateless features to TRAIN...\")\n",
    "    train_df = add_stateless_features(train_df)\n",
    "\n",
    "    print(\"Adding stateless features to TEST...\")\n",
    "    test_df = add_stateless_features(test_df)\n",
    "\n",
    "    print(\"✓ Stateless features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 3: Stateful Features & Leakage Prevention**\n",
    "Stateful features depend on the distribution of the data (e.g., \"How rare is this destination port?\").\n",
    "\n",
    "**Critical Rule:** To prevent data leakage, we calculate statistics (counts/frequencies) **ONLY** on the Training set. These learned statistics are then applied to the Test set. If a port appears in the Test set that was never seen in Training, it is correctly flagged as \"100% Rare\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STAGE 3] Stateful Features (Learned on Train, Applied to Test)...\n",
      "================================================================================\n",
      "Port Rarity: Found 49505 unique ports in training set\n",
      "   Unseen ports in test: 4,909 samples (marked as highly rare)\n",
      "✓ Stateful features added.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STAGE 3] Stateful Features (Learned on Train, Applied to Test)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    # 1. Port Rarity\n",
    "    if 'dst_port' in train_df.columns:\n",
    "        # Learn frequencies from TRAIN ONLY\n",
    "        port_counts = train_df['dst_port'].value_counts(normalize=True)\n",
    "        print(f\"Port Rarity: Found {len(port_counts)} unique ports in training set\")\n",
    "        \n",
    "        # Apply to Train\n",
    "        train_df['port_rarity'] = train_df['dst_port'].map(lambda x: 1.0 - port_counts.get(x, 0.0))\n",
    "        \n",
    "        # Apply to Test (Unseen ports get rarity 1.0)\n",
    "        test_df['port_rarity'] = test_df['dst_port'].map(lambda x: 1.0 - port_counts.get(x, 0.0))\n",
    "        \n",
    "        # Validation: Count unseen ports in test\n",
    "        unseen_test_ports = test_df[~test_df['dst_port'].isin(port_counts.index)].shape[0]\n",
    "        print(f\"   Unseen ports in test: {unseen_test_ports:,} samples (marked as highly rare)\")\n",
    "    else:\n",
    "        train_df['port_rarity'] = 0.5\n",
    "        test_df['port_rarity'] = 0.5\n",
    "        print(\"Port column not found. Using default rarity.\")\n",
    "\n",
    "    # 2. Protocol Rarity (if available)\n",
    "    if 'protocol' in train_df.columns:\n",
    "        proto_counts = train_df['protocol'].value_counts(normalize=True)\n",
    "        train_df['protocol_rarity'] = train_df['protocol'].map(lambda x: 1.0 - proto_counts.get(x, 0.0))\n",
    "        test_df['protocol_rarity'] = test_df['protocol'].map(lambda x: 1.0 - proto_counts.get(x, 0.0))\n",
    "    else:\n",
    "        train_df['protocol_rarity'] = 0.5\n",
    "        test_df['protocol_rarity'] = 0.5\n",
    "\n",
    "    print(\"✓ Stateful features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 4: Anomaly Detection (Isolation Forest)**\n",
    "We train an Isolation Forest model to create a meta-feature called `anomaly_score`.\n",
    "\n",
    "* **Training Data:** Benign traffic from the Training Set **only**.\n",
    "* **Goal:** Learn the \"normal\" baseline.\n",
    "* **Application:** The model scores both Train and Test data. High scores indicate deviation from the benign baseline (potential attacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STAGE 4] Isolation Forest Feature (Evasion Detection)...\n",
      "================================================================================\n",
      "Features used for Isolation Forest: 87\n",
      "Training Isolation Forest on 1,858,775 benign samples...\n",
      "Scoring Train set...\n",
      "Scoring Test set...\n",
      "\n",
      "✓ Anomaly Scores Generated\n",
      "   Train: mean=0.175\n",
      "   Test:  mean=0.175\n",
      "✓ Isolation Forest and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STAGE 4] Isolation Forest Feature (Evasion Detection)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    # Select numeric features for Isolation Forest\n",
    "    meta_cols = {'label', 'label_str', 'timestamp', 'source_file', 'src_ip', 'dst_ip'}\n",
    "    feature_cols = [c for c in train_df.columns if c not in meta_cols and np.issubdtype(train_df[c].dtype, np.number)]\n",
    "\n",
    "    print(f\"Features used for Isolation Forest: {len(feature_cols)}\")\n",
    "\n",
    "    # Train IF on BENIGN TRAIN data only\n",
    "    X_train_benign = train_df[train_df['label'] == 0][feature_cols].fillna(0)\n",
    "    print(f\"Training Isolation Forest on {len(X_train_benign):,} benign samples...\")\n",
    "\n",
    "    iso_forest = IsolationForest(contamination=0.01, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    iso_forest.fit(X_train_benign)\n",
    "\n",
    "    # Score Train and Test\n",
    "    print(\"Scoring Train set...\")\n",
    "    train_scores = -iso_forest.score_samples(train_df[feature_cols].fillna(0))\n",
    "\n",
    "    print(\"Scoring Test set...\")\n",
    "    test_scores = -iso_forest.score_samples(test_df[feature_cols].fillna(0))\n",
    "\n",
    "    # Normalize scores to [0, 1] for stability\n",
    "    scaler = MinMaxScaler()\n",
    "    train_df['anomaly_score'] = scaler.fit_transform(train_scores.reshape(-1, 1)).ravel()\n",
    "    test_df['anomaly_score'] = scaler.transform(test_scores.reshape(-1, 1)).ravel()\n",
    "\n",
    "    print(f\"\\n✓ Anomaly Scores Generated\")\n",
    "    print(f\"   Train: mean={train_df['anomaly_score'].mean():.3f}\")\n",
    "    print(f\"   Test:  mean={test_df['anomaly_score'].mean():.3f}\")\n",
    "\n",
    "    # Save artifacts for future inference\n",
    "    joblib.dump(iso_forest, os.path.join(ARTIFACTS_DIR, \"iso_forest_model.joblib\"))\n",
    "    joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, \"anomaly_scaler.joblib\"))\n",
    "    print(\"✓ Isolation Forest and scaler saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stage 5: Save Processed Data**\n",
    "The processed DataFrames are saved as **Parquet** files. Parquet is significantly faster and more storage-efficient than CSV, preserving data types (float/int) correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[STAGE 5] Saving processed Parquet files...\n",
      "================================================================================\n",
      "\n",
      "✓ Train Saved: C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\processed\\train_mon_thu.parquet\n",
      "   Shape: (2127498, 89)\n",
      "\n",
      "✓ Test Saved:  C:\\Users\\knand\\Desktop\\K-Nandu_Minor_Project_SOC_Triage_SSL\\processed\\test_friday.parquet\n",
      "   Shape: (703245, 89)\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "DATA LEAKAGE CHECK: ✓ ZERO LEAKAGE VALIDATED\n",
      " - Train/Test split done BEFORE processing\n",
      " - Port rarity learned from TRAIN only\n",
      " - Isolation Forest trained on BENIGN TRAIN data only\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STAGE 5] Saving processed Parquet files...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    train_out = os.path.join(OUTPUT_DIR, \"train_mon_thu.parquet\")\n",
    "    test_out = os.path.join(OUTPUT_DIR, \"test_friday.parquet\")\n",
    "\n",
    "    train_df.to_parquet(train_out, index=False)\n",
    "    test_df.to_parquet(test_out, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Train Saved: {train_out}\")\n",
    "    print(f\"   Shape: {train_df.shape}\")\n",
    "\n",
    "    print(f\"\\n✓ Test Saved:  {test_out}\")\n",
    "    print(f\"   Shape: {test_df.shape}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREPROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDATA LEAKAGE CHECK: ✓ ZERO LEAKAGE VALIDATED\")\n",
    "    print(f\" - Train/Test split done BEFORE processing\")\n",
    "    print(f\" - Port rarity learned from TRAIN only\")\n",
    "    print(f\" - Isolation Forest trained on BENIGN TRAIN data only\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
